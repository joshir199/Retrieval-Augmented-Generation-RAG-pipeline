{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A4Qp7hiEw7In"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#load environment variables\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYf331EKzAtv",
        "outputId": "3c978342-a1c0-4082-8f76-968d67d4b5e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "ENBnk2UKzAhA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result = chatmodel.invoke(\"What is special about India?\")"
      ],
      "metadata": {
        "id": "BIrh8I4e1vwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get only cotents of the result. It only contains the output message\n",
        "print(\"content only: \", chat_result.content)\n",
        "\n",
        "# To get full message from the chat model\n",
        "# It contains details of the prompt, model, prompt tokens, result tokens\n",
        "print(\"full result: \", chat_result)"
      ],
      "metadata": {
        "id": "gxhZ-7QYK9Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOHIoJRaOYPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Examples for basic conversation using langchain and openai model**"
      ],
      "metadata": {
        "id": "TBBLBFWjPMbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "# defining messages with its proper type\n",
        "messages = [\n",
        "    # setting the chatmodel behaviour for the response of the query\n",
        "    # It always come first, as it sets the context.\n",
        "    SystemMessage(content=\"You are an expert assistant.\"),\n",
        "    # setting the real query in message form\n",
        "    HumanMessage(content=\"Who won the world series in 2020?\")\n",
        "]"
      ],
      "metadata": {
        "id": "N5GhoMOOPcBF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_result = chatmodel.invoke(messages)\n",
        "print(\"result message: \", conv_result.content)"
      ],
      "metadata": {
        "id": "qSOYrvg7RStc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, inorder to keep the conversation going on for longer context window.\n",
        "PREV_CONTEXT_LIMIT = 10\n",
        "\n",
        "def get_conversation_result(messages, humanMessage, prevAIMessage):\n",
        "  if mCount >= PREV_CONTEXT_LIMIT:\n",
        "    mCount = 0\n",
        "    print(\"context limit crossed. Starting conversation with new context\")\n",
        "    while len(messages)>1:\n",
        "      messages.pop(0)\n",
        "    messages.append(humanMessage)\n",
        "    return AIMessage(content=chatmodel.invoke(messages).content), messages\n",
        "  else:\n",
        "    messages.append(prevAIMessage)\n",
        "    messages.append(humanMessage)\n",
        "    return AIMessage(content=chatmodel.invoke(messages).content), messages\n",
        "\n"
      ],
      "metadata": {
        "id": "6Dy11RPRSxvX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the loop of conversation for messages\n",
        "\n",
        "prevAIMessage = AIMessage(content=chatmodel.invoke(messages).content)\n",
        "\n",
        "while True:\n",
        "  query = input(\" User: \")\n",
        "  if query.lower() == \"exit\":\n",
        "    break\n",
        "  # get human message from user query only for each coversation loop as a form of input\n",
        "  humanMessage = HumanMessage(content=query)\n",
        "\n",
        "  # get chat result as well as updated messages\n",
        "  prevAIMessage, messages = get_conversation_result(messages, humanMessage, prevAIMessage)\n",
        "\n"
      ],
      "metadata": {
        "id": "DayP7UL_UxWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding the chat Prompt template for building chatbots**"
      ],
      "metadata": {
        "id": "VYHIOuf6fA2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatPrompt template helps in creating custom chat query with parameters.\n",
        "# It can be seen as masked prompt template.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chatTemplate = \"Explain this {topic} in details\"\n",
        "prompt_template = ChatPromptTemplate.from_template(chatTemplate)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"Machine Learning\"})\n",
        "\n",
        "chatResult = chatmodel.invoke([prompt])\n",
        "print(chatResult.content)"
      ],
      "metadata": {
        "id": "lJFEpUaxqku9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also change the context of the messages on runtime(dynamically)\n",
        "# Like changing the context of the SystemAI message\n",
        "\n",
        "advancedTemplate = ChatPromptTemplate.format_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert in the {topic}.\"),\n",
        "        (\"human\", \"Please explain me in details about the {subject} along with its pros & cons.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "advancedPrompt = advancedTemplate.invoke({\"topic\": \"Machine Learning\", \"subject\": \"Decision Trees\"})\n",
        "\n",
        "advancedResult = chatmodel.invoke(advancedPrompt)\n",
        "print(advancedResult.content)"
      ],
      "metadata": {
        "id": "DLTWgcAqtXW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBd7Yml1uyxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}