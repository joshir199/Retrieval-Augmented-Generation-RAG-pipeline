{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A4Qp7hiEw7In"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "#load environment variables\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYf331EKzAtv",
        "outputId": "94b92bea-d0e2-4c97-f9c4-df621fa878b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "ENBnk2UKzAhA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result = chatmodel.invoke(\"What is special about India?\")"
      ],
      "metadata": {
        "id": "BIrh8I4e1vwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get only cotents of the result. It only contains the output message\n",
        "print(\"content only: \", chat_result.content)\n",
        "\n",
        "# To get full message from the chat model\n",
        "# It contains details of the prompt, model, prompt tokens, result tokens\n",
        "print(\"full result: \", chat_result)"
      ],
      "metadata": {
        "id": "gxhZ-7QYK9Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOHIoJRaOYPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Examples for basic conversation using langchain and openai model**"
      ],
      "metadata": {
        "id": "TBBLBFWjPMbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "# defining messages with its proper type\n",
        "messages = [\n",
        "    # setting the chatmodel behaviour for the response of the query\n",
        "    # It always come first, as it sets the context.\n",
        "    SystemMessage(content=\"You are an expert assistant.\"),\n",
        "    # setting the real query in message form\n",
        "    HumanMessage(content=\"Who won the world series in 2020?\")\n",
        "]"
      ],
      "metadata": {
        "id": "N5GhoMOOPcBF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_result = chatmodel.invoke(messages)\n",
        "print(\"result message: \", conv_result.content)"
      ],
      "metadata": {
        "id": "qSOYrvg7RStc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, inorder to keep the conversation going on for longer context window.\n",
        "PREV_CONTEXT_LIMIT = 10\n",
        "\n",
        "def get_conversation_result(messages, humanMessage, prevAIMessage):\n",
        "  if mCount >= PREV_CONTEXT_LIMIT:\n",
        "    mCount = 0\n",
        "    print(\"context limit crossed. Starting conversation with new context\")\n",
        "    while len(messages)>1:\n",
        "      messages.pop(0)\n",
        "    messages.append(humanMessage)\n",
        "    return AIMessage(content=chatmodel.invoke(messages).content), messages\n",
        "  else:\n",
        "    messages.append(prevAIMessage)\n",
        "    messages.append(humanMessage)\n",
        "    return AIMessage(content=chatmodel.invoke(messages).content), messages\n",
        "\n"
      ],
      "metadata": {
        "id": "6Dy11RPRSxvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the loop of conversation for messages\n",
        "\n",
        "prevAIMessage = AIMessage(content=chatmodel.invoke(messages).content)\n",
        "\n",
        "while True:\n",
        "  query = input(\" User: \")\n",
        "  if query.lower() == \"exit\":\n",
        "    break\n",
        "  # get human message from user query only for each coversation loop as a form of input\n",
        "  humanMessage = HumanMessage(content=query)\n",
        "\n",
        "  # get chat result as well as updated messages\n",
        "  prevAIMessage, messages = get_conversation_result(messages, humanMessage, prevAIMessage)\n",
        "\n"
      ],
      "metadata": {
        "id": "DayP7UL_UxWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding the chat Prompt template for building chatbots**"
      ],
      "metadata": {
        "id": "VYHIOuf6fA2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatPrompt template helps in creating custom chat query with parameters.\n",
        "# It can be seen as masked prompt template.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "chatTemplate = \"Explain this {topic} in details\"\n",
        "prompt_template = ChatPromptTemplate.from_template(chatTemplate)\n",
        "\n"
      ],
      "metadata": {
        "id": "lJFEpUaxqku9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"topic\": \"Machine Learning\"})\n",
        "\n",
        "chatResult = chatmodel.invoke([prompt])\n",
        "print(chatResult.content)"
      ],
      "metadata": {
        "id": "CVQZvm2PxjZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also change the context of the messages on runtime(dynamically)\n",
        "# Like changing the context of the SystemAI message\n",
        "\n",
        "advancedTemplate = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert in the {topic}.\"),\n",
        "        (\"human\", \"Please explain me in details about the {subject} along with its pros & cons.\")\n",
        "    ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "DLTWgcAqtXW8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "advancedPrompt = advancedTemplate.invoke({\"topic\": \"Machine Learning\", \"subject\": \"Decision Trees\"})\n",
        "\n",
        "advancedResult = chatmodel.invoke(advancedPrompt)\n",
        "print(advancedResult.content)"
      ],
      "metadata": {
        "id": "z1C0kK4BxmPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5UPP8uav1Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chaining of the langchain components**\n",
        "\n",
        "Since, each langchain components are Runnable instances, where we can call invoke() calls. Thus, we can compose these runnables using (|) pipe operator. The output of previous component will be taken as input for next component in a sequential manner.\n",
        "\n",
        "This property is achieved due to LangChain Expression Language (LCEL).\n"
      ],
      "metadata": {
        "id": "Lxo-hQFnvzQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating chain with prompt and chat model\n",
        "\n",
        "chain1 = prompt_template | chatmodel\n",
        "\n",
        "chain2 = advancedTemplate | chatmodel\n"
      ],
      "metadata": {
        "id": "RBd7Yml1uyxZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# executing chains\n",
        "\n",
        "resut1 = chain1.invoke({\"topic\": \"Machine Learning and AI\"})\n",
        "\n",
        "result2 = chain2.invoke({\"topic\": \"Machine Learning and AI\", \"subject\": \"Gaussian Splatting\"})"
      ],
      "metadata": {
        "id": "y91cfc8txQKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The output from above is in langshain component - AIMessage(.......)\n",
        "# In order to get raw string output, we can use String OutputParser component of\n",
        "# langchain.\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "string_result1 = chain1 | StrOutputParser()\n",
        "string_result2 = chain2 | StrOutputParser()\n",
        "\n",
        "# Note: above statement is like: prompt_template | chatmodel | StrOutputParser()\n",
        "\n",
        "\n",
        "# Now, we can execute it using invoke function\n",
        "\n",
        "output1 = string_result1.invoke({\"topic\": \"Machine Learning and AI\"})\n",
        "\n",
        "output2 = string_result2.invoke({\"topic\": \"Machine Learning and AI\", \"subject\": \"Gaussian Splatting\"})\n",
        "\n",
        "print(\"\\n result 1:  \", output1)\n",
        "print(\"\\n result 2:  \", output2)"
      ],
      "metadata": {
        "id": "oyVWw7HXyaEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating more complex and parallel branched version of langChain\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "\n",
        "branch1 = (\n",
        "    RunnableLambda(lambda x: x.upper()) | StrOutputParser()\n",
        ")\n",
        "branch2 = (\n",
        "    RunnableLambda(lambda x: x.lower()) | StrOutputParser()\n",
        ")\n",
        "\n",
        "full_flow = (\n",
        "    advancedTemplate\n",
        "    | chatmodel\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"upper\": branch1, \"lower\": branch2})\n",
        "    | RunnableLambda(lambda x: print(\" output value: \", x[\"upper\"] + x[\"lower\"]))\n",
        ")\n"
      ],
      "metadata": {
        "id": "LFYtX3Dgy5qy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing full flow\n",
        "\n",
        "full_flow_result = full_flow.invoke({\"topic\": \"Machine Learning and AI\", \"subject\": \"Gaussian Splatting\"})\n",
        "\n",
        "print(full_flow_result)"
      ],
      "metadata": {
        "id": "uZ8YsyKKFeE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding RunnableBranch for Conditional branches\n",
        "# Taking an example of topic explanation based on user query\n",
        "\n",
        "mathematical_explanation = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are expert in AI concepts\"),\n",
        "    (\"human\", \"Generate the explanation about given {topic} in mathematical way only\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "theoretical_explanation = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are expert in AI concepts\"),\n",
        "    (\"human\", \"Generate the explanation about given {topic} in theoretical way only\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "combined_explanation = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are expert in AI concepts\"),\n",
        "    (\"human\", \"Generate the explanation about given {topic} in both mathematical and theoretical way\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "vYonn5JoHP1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating branches with condition\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "branches_values = RunnableBranch(\n",
        "    (lambda x: \"mathematical\" in x[\"topic\"].lower(), mathematical_explanation | chatmodel | StrOutputParser()),\n",
        "    (lambda x: \"theoretical\" in x[\"topic\"].lower(), theoretical_explanation | chatmodel | StrOutputParser()),\n",
        "    (lambda x: True, combined_explanation | chatmodel | StrOutputParser())\n",
        ")\n",
        "\n",
        "branch_flow_result = branches_values.invoke({\"topic\": \"Machine Learning and AI with mathematical description\"})\n",
        "\n",
        "print(branch_flow_result)"
      ],
      "metadata": {
        "id": "iL3Cojw3OilD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}