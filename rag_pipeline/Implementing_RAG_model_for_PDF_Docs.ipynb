{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd4-Oz0QnHeE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing RAG pipeline for querying and extracting relevant information from list of PDF docs**\n",
        "\n",
        "Due to security/privacy concern over using Chatmodels on sensitive datas, It is not recommended to train the model using private sensitive datas.\n",
        "\n",
        "Therefore, RAG pipeline is used for extracting relevant infos from sensitive datas, without requiring to train the model again.\n",
        "\n"
      ],
      "metadata": {
        "id": "iKPUvmp9nXbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be using langchain framework to build end-to-end pipeline for RAG flow.\n",
        "# langchain is most popular framework to building LLM usecase pipelines.\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "-79jds0noa5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import chains, vectorstores\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "import PyPDF2\n",
        "import pypdf\n",
        "import re\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "2ZjUb8Kto1l6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing the Documents and storing it into **VectorStores** Databases.\n",
        "\n",
        "These documents will be converted into fixed sized chunks and then stored in document embedded forms.\n",
        "\n",
        "This way, it will be easy to retrieve the required documents."
      ],
      "metadata": {
        "id": "4TWWKIhnqmRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_document_load = PyPDF2.PdfReader(open(\"/content/sell_my_dream.pdf\", \"rb\"))"
      ],
      "metadata": {
        "id": "iDsDl-u1qWBr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pdf_document_load.pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZyFxHPxuEZd",
        "outputId": "dfe09407-5fd7-4eca-841d-f7aa3c425d58"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start building pipelines be intiating the model\n",
        "\n",
        "# load enviroment variables and API_KEYs\n",
        "load_dotenv()\n",
        "\n",
        "# Instantiate chat model\n",
        "def init_chatmodel():\n",
        "  chatmodel = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "  return chatmodel\n",
        "\n",
        "# instantiate Embeddings\n",
        "def init_embeddings():\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "  return embeddings\n",
        "\n",
        "# load the pdf documents and load its contents for further processing\n",
        "def load_pdf_document(pdfPath):\n",
        "  pdf_document = PyPDFLoader(pdfPath)\n",
        "  documents = pdf_document.load()\n",
        "  return documents\n",
        "\n",
        "# split the documents into multiple chunks(of size 1000 tokens) for indexing\n",
        "def split_pdf_documents(documents):\n",
        "  doc_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "  docs = doc_splitter.split_documents(documents)\n",
        "  return docs"
      ],
      "metadata": {
        "id": "oFpYc4SCuhi7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, document embedding is to be done using BERT based OpenAIEmbeddings\n",
        "\"\"\"\n",
        "This method leverages the power of a OpenAIEmbeddings model for embedding generation\n",
        "and the efficiency of Chroma for storing and querying embeddings, facilitating\n",
        "various downstream tasks like document retrieval and clustering.\n",
        "ChromaDB then adds each document's embedding to the collection using unique\n",
        "document identifiers\n",
        "\"\"\"\n",
        "\n",
        "def create_vectorDB_store(textdocs, embeddingFun):\n",
        "  vectorDB = vectorstores.Chroma.from_documents(textdocs, embeddingFun)\n",
        "  return vectorDB\n"
      ],
      "metadata": {
        "id": "ByFa_7O4wt70"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Part\n",
        "\"\"\"\n",
        "Given a query and documents, this method will fetch the relevant documents\n",
        "based on similarity search.\n",
        "\"\"\"\n",
        "\n",
        "# It can produce n number of relevant documents chunks requested by user.\n",
        "\n",
        "def get_retrieved_docs(n_documents, stored_VectorDB):\n",
        "  retrieved_docs = stored_VectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":n_documents})\n",
        "  return retrieved_docs"
      ],
      "metadata": {
        "id": "M4bjEZNa49GQ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining all components to create the RAG pipeline"
      ],
      "metadata": {
        "id": "iCcfW_cB7aZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrate all components into RAG pipeline for end-to-end chain flow\n",
        "\n",
        "loaded_docs = load_pdf_document(\"/content/sell_my_dream.pdf\")\n",
        "textdocs = split_pdf_documents(loaded_docs)\n",
        "print(\"chunk size: \", len(textdocs))\n",
        "vectordb = create_vectorDB_store(textdocs, embeddingFun=init_embeddings())\n",
        "retrievers = get_retrieved_docs(3, vectordb)\n"
      ],
      "metadata": {
        "id": "P0HdRzzS6dBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f961eb-bee3-4ca6-c126-63b21c402926"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk size:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get relevant docs as per query\n",
        "query = \"Why does the author compare Neruda to a Renaissance pope?\"\n",
        "relevant_docs = retrievers.invoke(query)"
      ],
      "metadata": {
        "id": "nc-6yPo2ofsL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print those relevant docs\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "  print(f\"Document {i}: \\n {doc.metadata} \")\n",
        "  print(\"\\n-------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRVV0kLmofhX",
        "outputId": "ecbfbd52-7200-4219-df81-be1301ceca86"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: \n",
            " {'page': 2, 'source': '/content/sell_my_dream.pdf'} \n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "Document 2: \n",
            " {'page': 2, 'source': '/content/sell_my_dream.pdf'} \n",
            "\n",
            "-------------------------------------------------------\n",
            "\n",
            "Document 3: \n",
            " {'page': 3, 'source': '/content/sell_my_dream.pdf'} \n",
            "\n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined chain with LLM model in it.\n",
        "chatmodel = init_chatmodel()\n",
        "\n",
        "rag_chain = chains.RetrievalQA.from_chain_type(\n",
        "    llm=chatmodel,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retrievers,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "O8ruS9QKofUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling RAG pipeline with query:\n",
        "query_result1 = rag_chain.invoke({\"query\": \"Why does the author compare Neruda to a Renaissance pope?\"})\n",
        "\n",
        "print(\"answer: \", query_result1[\"result\"])"
      ],
      "metadata": {
        "id": "IqA8G80wBn4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also know which documents consitutes the answer:\n",
        "for document in query_result1[\"source_documents\"]:\n",
        "  print(re.sub(r\"\\s+\", \" \", document.page_content.strip()))\n",
        "  print(\"\\n-------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "lLIC1eJqCWcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up memory context for Chat of previous query.\n",
        "chat_history1 = [(query_result1[\"query\"], query_result1[\"result\"])]\n",
        "query_result2 = rag_chain.invoke({\n",
        "    \"query\": \"Are there any other reasons of comparing Neruda to a Renaissance pope?\",\n",
        "    \"chat_history\": chat_history1\n",
        "})\n",
        "print(\"answer: \", query_result2[\"result\"])"
      ],
      "metadata": {
        "id": "tZXQNYtDIXQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for document in query_result2[\"source_documents\"]:\n",
        "  print(re.sub(r\"\\s+\", \" \", document.page_content.strip()))\n",
        "  print(\"\\n-------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "WM9QyfYRI_C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLUnHobBb61U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e8HJEHQBqlQc"
      }
    }
  ]
}